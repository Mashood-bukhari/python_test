{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              NULL|                         NULL|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              NULL|                         NULL|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              NULL|                         NULL|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              NULL|                         NULL|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"GlobalTemperatures\").getOrCreate()\n",
    "\n",
    "file_path = \"GlobalLandTemperaturesByCity.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01|14.050999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|            16.082|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = df.dropna()\n",
    "\n",
    "columns_to_drop = [\"AverageTemperatureUncertainty\"]\n",
    "cleaned_df = cleaned_df.drop(*columns_to_drop)\n",
    "\n",
    "cleaned_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01|14.050999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|            16.082|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "transformed_df = cleaned_df.withColumn(\"dt\", col(\"dt\").cast(DateType()))\n",
    "transformed_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------+-------------+--------+---------+\n",
      "|        dt|AverageTemperature|   City|      Country|Latitude|Longitude|\n",
      "+----------+------------------+-------+-------------+--------+---------+\n",
      "|2000-01-01| 8.039000000000001|Abilene|United States|  32.95N|  100.53W|\n",
      "|2000-02-01|            11.908|Abilene|United States|  32.95N|  100.53W|\n",
      "|2000-03-01|            14.423|Abilene|United States|  32.95N|  100.53W|\n",
      "|2000-04-01|            18.274|Abilene|United States|  32.95N|  100.53W|\n",
      "|2000-05-01|            25.358|Abilene|United States|  32.95N|  100.53W|\n",
      "+----------+------------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = transformed_df.filter((col(\"Country\") == \"United States\") & (col(\"dt\").cast(\"string\").substr(1, 4) >= \"2000\"))\n",
    "filtered_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------+-------------+--------+---------+----+-----+---+\n",
      "|        dt|AverageTemperature|   City|      Country|Latitude|Longitude|Year|Month|Day|\n",
      "+----------+------------------+-------+-------------+--------+---------+----+-----+---+\n",
      "|2000-01-01| 8.039000000000001|Abilene|United States|  32.95N|  100.53W|2000|    1|  1|\n",
      "|2000-02-01|            11.908|Abilene|United States|  32.95N|  100.53W|2000|    2|  1|\n",
      "|2000-03-01|            14.423|Abilene|United States|  32.95N|  100.53W|2000|    3|  1|\n",
      "|2000-04-01|            18.274|Abilene|United States|  32.95N|  100.53W|2000|    4|  1|\n",
      "|2000-05-01|            25.358|Abilene|United States|  32.95N|  100.53W|2000|    5|  1|\n",
      "+----------+------------------+-------+-------------+--------+---------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "final_df = filtered_df.withColumn(\"Year\", year(col(\"dt\"))) \\\n",
    "                      .withColumn(\"Month\", month(col(\"dt\"))) \\\n",
    "                      .withColumn(\"Day\", dayofmonth(col(\"dt\")))\n",
    "final_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|         City|    AvgTemperature|\n",
      "+-------------+------------------+\n",
      "|   Charleston|19.514345454545456|\n",
      "|       Corona|16.823933333333333|\n",
      "|Coral Springs|23.816096969696957|\n",
      "|    Anchorage|-1.013993902439024|\n",
      "|    Allentown|10.888254545454545|\n",
      "+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_temp_by_city = final_df.groupBy(\"City\").avg(\"AverageTemperature\").withColumnRenamed(\"avg(AverageTemperature)\", \"AvgTemperature\")\n",
    "avg_temp_by_city.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|            City|    AvgTemperature|\n",
      "+----------------+------------------+\n",
      "|Port Saint Lucie|23.816096969696957|\n",
      "|  Pembroke Pines|23.816096969696957|\n",
      "| Fort Lauderdale|23.816096969696957|\n",
      "|   Coral Springs|23.816096969696957|\n",
      "|         Hialeah|23.816096969696957|\n",
      "+----------------+------------------+\n",
      "\n",
      "+-----------+------------------+\n",
      "|       City|    AvgTemperature|\n",
      "+-----------+------------------+\n",
      "|  Anchorage|-1.013993902439024|\n",
      "|     Arvada| 3.391303030303032|\n",
      "|Minneapolis| 6.432606060606066|\n",
      "| Saint Paul| 6.432606060606066|\n",
      "|Sioux Falls|7.7314303030303035|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "hottest_cities = avg_temp_by_city.orderBy(desc(\"AvgTemperature\")).limit(5)\n",
    "hottest_cities.show()\n",
    "\n",
    "coldest_cities = avg_temp_by_city.orderBy(asc(\"AvgTemperature\")).limit(5)\n",
    "coldest_cities.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|Year|YearlyAvgTemperature|\n",
      "+----+--------------------+\n",
      "|2003|               9.836|\n",
      "|2007|  10.627333333333334|\n",
      "|2006|            11.51925|\n",
      "|2013|  12.163888888888886|\n",
      "|2004|             10.3895|\n",
      "|2012|             11.9715|\n",
      "|2009|  10.141833333333334|\n",
      "|2001|  10.930999999999997|\n",
      "|2005|  10.681416666666665|\n",
      "|2000|   9.969083333333334|\n",
      "|2010|  11.357583333333332|\n",
      "|2011|            11.27225|\n",
      "|2008|  10.641666666666666|\n",
      "|2002|  11.252166666666668|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ny_avg_temp = final_df.filter(col(\"City\") == \"New York\").groupBy(\"Year\").avg(\"AverageTemperature\").withColumnRenamed(\"avg(AverageTemperature)\", \"YearlyAvgTemperature\")\n",
    "ny_avg_temp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hottest_cities_data = hottest_cities.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(hottest_cities_data[\"City\"], hottest_cities_data[\"AvgTemperature\"], color='red')\n",
    "plt.title(\"Top 5 Hottest Cities\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Average Temperature\")\n",
    "plt.savefig(\"hottest_cities_chart.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.partitionBy(\"Day\").csv(\"output/cleaned_data\", header=True, mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
